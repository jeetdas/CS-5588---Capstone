{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.0.335 in ./.conda/lib/python3.10/site-packages (0.0.335)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.10/site-packages (from langchain==0.0.335) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.conda/lib/python3.10/site-packages (from langchain==0.0.335) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.conda/lib/python3.10/site-packages (from langchain==0.0.335) (3.9.1)\n",
      "Requirement already satisfied: anyio<4.0 in ./.conda/lib/python3.10/site-packages (from langchain==0.0.335) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.conda/lib/python3.10/site-packages (from langchain==0.0.335) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.conda/lib/python3.10/site-packages (from langchain==0.0.335) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.10/site-packages (from langchain==0.0.335) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in ./.conda/lib/python3.10/site-packages (from langchain==0.0.335) (0.0.69)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.conda/lib/python3.10/site-packages (from langchain==0.0.335) (1.26.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.conda/lib/python3.10/site-packages (from langchain==0.0.335) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.conda/lib/python3.10/site-packages (from langchain==0.0.335) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.conda/lib/python3.10/site-packages (from langchain==0.0.335) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.335) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.335) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.335) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.335) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.335) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.335) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.335) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in ./.conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.335) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.335) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.335) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.335) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.0.335) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in ./.conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.0.335) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./.conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.0.335) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.335) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.335) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.335) (2023.11.17)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.335) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.335) (1.0.0)\n",
      "Collecting openai==1.3.0\n",
      "  Downloading openai-1.3.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in ./.conda/lib/python3.10/site-packages (from openai==1.3.0) (3.7.1)\n",
      "Collecting distro<2,>=1.7.0 (from openai==1.3.0)\n",
      "  Using cached distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai==1.3.0)\n",
      "  Using cached httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.conda/lib/python3.10/site-packages (from openai==1.3.0) (2.5.2)\n",
      "Requirement already satisfied: tqdm>4 in ./.conda/lib/python3.10/site-packages (from openai==1.3.0) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in ./.conda/lib/python3.10/site-packages (from openai==1.3.0) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./.conda/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai==1.3.0) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.conda/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai==1.3.0) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in ./.conda/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai==1.3.0) (1.2.0)\n",
      "Requirement already satisfied: certifi in ./.conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.3.0) (2023.11.17)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.3.0)\n",
      "  Using cached httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.3.0)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.3.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in ./.conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.3.0) (2.14.5)\n",
      "Downloading openai-1.3.0-py3-none-any.whl (220 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mMB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached httpx-0.25.2-py3-none-any.whl (74 kB)\n",
      "Using cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: h11, distro, httpcore, httpx, openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.0\n",
      "    Uninstalling openai-0.28.0:\n",
      "      Successfully uninstalled openai-0.28.0\n",
      "Successfully installed distro-1.8.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.0\n",
      "Requirement already satisfied: tiktoken in ./.conda/lib/python3.10/site-packages (0.5.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.conda/lib/python3.10/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
      "Requirement already satisfied: PyPDF2 in ./.conda/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: faiss-cpu in ./.conda/lib/python3.10/site-packages (1.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.0.335\n",
    "!pip install openai==1.3.0\n",
    "!pip install tiktoken\n",
    "!pip install PyPDF2\n",
    "!pip install faiss-cpu\n",
    "!pip install -q nougat-ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import io\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "pdf_reader = PdfReader(\"ASCE_7-16_1.pdf\")\n",
    "for page in pdf_reader.pages:\n",
    "  text += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASCE STANDARD\n",
      "ASCE/SEI\n",
      "7-16\n",
      "Minimum Design Loads and \n",
      "Associated Criteria for \n",
      "Buildings and Other Structures\n",
      "Downloaded from ascelibrary.org by University of Illinois At Urbana on 10/07/19. Copyright ASCE. For personal use only; all rights reserved.\n",
      "ASCE STANDARD ASCE/SEI\n",
      "7-16\n",
      "Minimum Design Loads and\n",
      "Associated Criteria for Buildingsand Other Structures\n",
      "PUBLISHED BY THE AMERICAN SOCIETY OF CIVIL ENGINEERS\n",
      "Downloaded from ascelibrary.org by University of Illinois At Urbana on 10/07/19. Copyright ASCE. For personal use only; all rights reserved.\n",
      "Library of Congress Cataloging-in-Publication Data\n",
      "Names: American Society of Civil Engineers.\n",
      "Title: Minimum design loads and associated criteria for buildings and other structures.\n",
      "Other titles: Minimum design loads for buildings and other structures. | ASCE standard, ASCE/\n",
      "SEI 7-16, minimum design loads and associated criteria for buildings and other structures\n",
      "Description: Reston, Virginia : American Society of Civil Engineers, [2017] | Ear\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading nougat checkpoint version 0.1.0-small to path /Users/jeetdas/.cache/torch/hub/nougat-0.1.0-small\n",
      "config.json: 100%|██████████████████████████████| 557/557 [00:00<00:00, 922kb/s]\n",
      "pytorch_model.bin: 100%|█████████████████████| 956M/956M [01:01<00:00, 16.3Mb/s]\n",
      "special_tokens_map.json: 100%|████████████████| 96.0/96.0 [00:00<00:00, 348kb/s]\n",
      "tokenizer.json: 100%|██████████████████████| 2.04M/2.04M [00:00<00:00, 16.5Mb/s]\n",
      "tokenizer_config.json: 100%|████████████████████| 106/106 [00:00<00:00, 241kb/s]\n",
      "/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "  0%|                                                   | 0/223 [00:00<?, ?it/s][nltk_data] Downloading package words to /Users/jeetdas/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "INFO:root:Processing file ASCE_7-16_1.pdf with 889 pages\n",
      "  0%|▏                                      | 1/223 [06:48<25:10:29, 408.24s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/bin/nougat\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/predict.py\", line 167, in main\n",
      "    model_output = model.inference(\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/nougat/model.py\", line 580, in inference\n",
      "    last_hidden_state = self.encoder(image_tensors)\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/nougat/model.py\", line 123, in forward\n",
      "    x = self.model.layers(x)\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/timm/models/swin_transformer.py\", line 413, in forward\n",
      "    x = blk(x)\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/timm/models/swin_transformer.py\", line 295, in forward\n",
      "    attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jeetdas/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/timm/models/swin_transformer.py\", line 200, in forward\n",
      "    x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
      "RuntimeError: MPS backend out of memory (MPS allocated: 10.22 GB, other allocations: 7.93 GB, max allowed: 18.13 GB). Tried to allocate 9.19 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
      "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!nougat 'ASCE_7-16_1.pdf' --out ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_filename = \"ASCE_7-16_56-60.mmd\"\n",
    "mmd_file = open(mmd_filename, \"r\")\n",
    "mmd_text = mmd_file.read()\n",
    "print(mmd_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1356, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "chunks = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-lDtgh4Rd6ovov1G7yFXjXH7P on tokens per min (TPM): Limit 1000000, Used 754921, Requested 262322. Please try again in 1.034s. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_texts(texts=chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, async_client=None, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, http_client=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = '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'\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'OpenAI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/jeetdas/Workspace/CS-5588---Capstone/exp1.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jeetdas/Workspace/CS-5588---Capstone/exp1.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm \u001b[39m=\u001b[39m ChatOpenAI()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jeetdas/Workspace/CS-5588---Capstone/exp1.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m memory \u001b[39m=\u001b[39m ConversationBufferMemory(memory_key\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m'\u001b[39m, return_messages\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jeetdas/Workspace/CS-5588---Capstone/exp1.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m conversation_chain \u001b[39m=\u001b[39m ConversationalRetrievalChain\u001b[39m.\u001b[39mfrom_llm(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jeetdas/Workspace/CS-5588---Capstone/exp1.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     llm\u001b[39m=\u001b[39mllm,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jeetdas/Workspace/CS-5588---Capstone/exp1.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     retriever\u001b[39m=\u001b[39mvectorstore\u001b[39m.\u001b[39mas_retriever(),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jeetdas/Workspace/CS-5588---Capstone/exp1.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     memory\u001b[39m=\u001b[39mmemory\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jeetdas/Workspace/CS-5588---Capstone/exp1.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Workspace/CS-5588---Capstone/.conda/lib/python3.10/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/Workspace/CS-5588---Capstone/.conda/lib/python3.10/site-packages/pydantic/v1/main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m, data)\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[1;32m    341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n",
      "File \u001b[0;32m~/Workspace/CS-5588---Capstone/.conda/lib/python3.10/site-packages/pydantic/v1/main.py:1102\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1102\u001b[0m     values \u001b[39m=\u001b[39m validator(cls_, values)\n\u001b[1;32m   1103\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m, \u001b[39mAssertionError\u001b[39;00m) \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m   1104\u001b[0m     errors\u001b[39m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[39m=\u001b[39mROOT_KEY))\n",
      "File \u001b[0;32m~/Workspace/CS-5588---Capstone/.conda/lib/python3.10/site-packages/langchain/chat_models/openai.py:310\u001b[0m, in \u001b[0;36mChatOpenAI.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m    300\u001b[0m     client_params \u001b[39m=\u001b[39m {\n\u001b[1;32m    301\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mapi_key\u001b[39m\u001b[39m\"\u001b[39m: values[\u001b[39m\"\u001b[39m\u001b[39mopenai_api_key\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    302\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39morganization\u001b[39m\u001b[39m\"\u001b[39m: values[\u001b[39m\"\u001b[39m\u001b[39mopenai_organization\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttp_client\u001b[39m\u001b[39m\"\u001b[39m: values[\u001b[39m\"\u001b[39m\u001b[39mhttp_client\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    309\u001b[0m     }\n\u001b[0;32m--> 310\u001b[0m     values[\u001b[39m\"\u001b[39m\u001b[39mclient\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mOpenAI(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mclient_params)\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mcompletions\n\u001b[1;32m    311\u001b[0m     values[\u001b[39m\"\u001b[39m\u001b[39masync_client\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mAsyncOpenAI(\n\u001b[1;32m    312\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mclient_params\n\u001b[1;32m    313\u001b[0m     )\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mcompletions\n\u001b[1;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'OpenAI'"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI()\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is this book about?', 'chat_history': [HumanMessage(content='What is this book about?'), AIMessage(content=\"I'm sorry, but I don't have enough information to determine what specific book you are referring to. Could you provide more context or a specific title?\")], 'answer': \"I'm sorry, but I don't have enough information to determine what specific book you are referring to. Could you provide more context or a specific title?\"}\n",
      "\n",
      "--------------------------------------------------\n",
      " I'm sorry, but I don't have enough information to determine what specific book you are referring to. Could you provide more context or a specific title?\n"
     ]
    }
   ],
   "source": [
    "response = conversation_chain({'question': \"What is this book about?\"})\n",
    "print(response)\n",
    "print(\"\\n--------------------------------------------------\\n\", response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def askQuestion(question, debug=False):\n",
    "  response = conversation_chain({'question': question})\n",
    "  if debug:\n",
    "    print(response[\"answer\"])\n",
    "  minResponse = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": 'Summarize this answer in the least number of words possible. Can be just one word.'\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f'Question: {question}'\n",
    "      }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=1200\n",
    "  )\n",
    "  finalResponse = minResponse.choices[0][\"message\"][\"content\"]\n",
    "  return finalResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'error'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/jeetdas/Workspace/CS-5588---Capstone/exp1.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jeetdas/Workspace/CS-5588---Capstone/exp1.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m res \u001b[39m=\u001b[39m askQuestion(\u001b[39m\"\u001b[39;49m\u001b[39mWhat is the amount of uniform live load I should apply to handrail and guardrail systems?\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jeetdas/Workspace/CS-5588---Capstone/exp1.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(res)\n",
      "\u001b[1;32m/Users/jeetdas/Workspace/CS-5588---Capstone/exp1.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jeetdas/Workspace/CS-5588---Capstone/exp1.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maskQuestion\u001b[39m(question, debug\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jeetdas/Workspace/CS-5588---Capstone/exp1.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m   response \u001b[39m=\u001b[39m conversation_chain({\u001b[39m'\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m'\u001b[39;49m: question})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jeetdas/Workspace/CS-5588---Capstone/exp1.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m   \u001b[39mif\u001b[39;00m debug:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jeetdas/Workspace/CS-5588---Capstone/exp1.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(response[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/langchain/chains/base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 312\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    314\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    316\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/langchain/chains/base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    299\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    300\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    301\u001b[0m     inputs,\n\u001b[1;32m    302\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 306\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    307\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    308\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/langchain/chains/conversational_retrieval/base.py:151\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    147\u001b[0m accepts_run_manager \u001b[39m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_docs)\u001b[39m.\u001b[39mparameters\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m accepts_run_manager:\n\u001b[0;32m--> 151\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_docs(new_question, inputs, run_manager\u001b[39m=\u001b[39;49m_run_manager)\n\u001b[1;32m    152\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_docs(new_question, inputs)  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/langchain/chains/conversational_retrieval/base.py:313\u001b[0m, in \u001b[0;36mConversationalRetrievalChain._get_docs\u001b[0;34m(self, question, inputs, run_manager)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_docs\u001b[39m(\n\u001b[1;32m    306\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    307\u001b[0m     question: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m     run_manager: CallbackManagerForChainRun,\n\u001b[1;32m    311\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m    312\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get docs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretriever\u001b[39m.\u001b[39;49mget_relevant_documents(\n\u001b[1;32m    314\u001b[0m         question, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child()\n\u001b[1;32m    315\u001b[0m     )\n\u001b[1;32m    316\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reduce_tokens_below_limit(docs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/langchain_core/retrievers.py:211\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    210\u001b[0m     run_manager\u001b[39m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 211\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     run_manager\u001b[39m.\u001b[39mon_retriever_end(\n\u001b[1;32m    214\u001b[0m         result,\n\u001b[1;32m    215\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/langchain_core/retrievers.py:204\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m _kwargs \u001b[39m=\u001b[39m kwargs \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expects_other_args \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 204\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_relevant_documents(\n\u001b[1;32m    205\u001b[0m         query, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwargs\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_relevant_documents(query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/langchain_core/vectorstores.py:656\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_relevant_documents\u001b[39m(\n\u001b[1;32m    653\u001b[0m     \u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[1;32m    654\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m    655\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msimilarity\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 656\u001b[0m         docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectorstore\u001b[39m.\u001b[39;49msimilarity_search(query, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msearch_kwargs)\n\u001b[1;32m    657\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msimilarity_score_threshold\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    658\u001b[0m         docs_and_similarities \u001b[39m=\u001b[39m (\n\u001b[1;32m    659\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorstore\u001b[39m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[1;32m    660\u001b[0m                 query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_kwargs\n\u001b[1;32m    661\u001b[0m             )\n\u001b[1;32m    662\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/langchain_community/vectorstores/faiss.py:512\u001b[0m, in \u001b[0;36mFAISS.similarity_search\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilarity_search\u001b[39m(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    494\u001b[0m     query: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    499\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m    500\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \n\u001b[1;32m    502\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 512\u001b[0m     docs_and_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msimilarity_search_with_score(\n\u001b[1;32m    513\u001b[0m         query, k, \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mfilter\u001b[39;49m, fetch_k\u001b[39m=\u001b[39;49mfetch_k, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    514\u001b[0m     )\n\u001b[1;32m    515\u001b[0m     \u001b[39mreturn\u001b[39;00m [doc \u001b[39mfor\u001b[39;00m doc, _ \u001b[39min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/langchain_community/vectorstores/faiss.py:393\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilarity_search_with_score\u001b[39m(\n\u001b[1;32m    373\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    374\u001b[0m     query: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    379\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Tuple[Document, \u001b[39mfloat\u001b[39m]]:\n\u001b[1;32m    380\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \n\u001b[1;32m    382\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39m        L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embed_query(query)\n\u001b[1;32m    394\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimilarity_search_with_score_by_vector(\n\u001b[1;32m    395\u001b[0m         embedding,\n\u001b[1;32m    396\u001b[0m         k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    400\u001b[0m     )\n\u001b[1;32m    401\u001b[0m     \u001b[39mreturn\u001b[39;00m docs\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/langchain_community/vectorstores/faiss.py:156\u001b[0m, in \u001b[0;36mFAISS._embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_embed_query\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m    155\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_function, Embeddings):\n\u001b[0;32m--> 156\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_function\u001b[39m.\u001b[39;49membed_query(text)\n\u001b[1;32m    157\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_function(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/langchain_community/embeddings/openai.py:696\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed_query\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m    688\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \n\u001b[1;32m    690\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[39m        Embedding for the text.\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 696\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_documents([text])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/langchain_community/embeddings/openai.py:667\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    666\u001b[0m engine \u001b[39m=\u001b[39m cast(\u001b[39mstr\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeployment)\n\u001b[0;32m--> 667\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/langchain_community/embeddings/openai.py:493\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    491\u001b[0m batched_embeddings: List[List[\u001b[39mfloat\u001b[39m]] \u001b[39m=\u001b[39m []\n\u001b[1;32m    492\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m _iter:\n\u001b[0;32m--> 493\u001b[0m     response \u001b[39m=\u001b[39m embed_with_retry(\n\u001b[1;32m    494\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    495\u001b[0m         \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtokens[i : i \u001b[39m+\u001b[39;49m _chunk_size],\n\u001b[1;32m    496\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_invocation_params,\n\u001b[1;32m    497\u001b[0m     )\n\u001b[1;32m    498\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    499\u001b[0m         response \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mdict()\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/langchain_community/embeddings/openai.py:116\u001b[0m, in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m _is_openai_v1():\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m embeddings\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 116\u001b[0m retry_decorator \u001b[39m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[1;32m    118\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_embed_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    120\u001b[0m     response \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_hw5_1/lib/python3.8/site-packages/langchain_community/embeddings/openai.py:56\u001b[0m, in \u001b[0;36m_create_retry_decorator\u001b[0;34m(embeddings)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenai\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# Wait 2^x * 1 second between each retry starting with\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# retry_min_seconds seconds, then up to retry_max_seconds seconds,\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# then retry_max_seconds seconds afterwards\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m# retry_min_seconds and retry_max_seconds are optional arguments of\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m# OpenAIEmbeddings\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mreturn\u001b[39;00m retry(\n\u001b[1;32m     48\u001b[0m     reraise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m     stop\u001b[39m=\u001b[39mstop_after_attempt(embeddings\u001b[39m.\u001b[39mmax_retries),\n\u001b[1;32m     50\u001b[0m     wait\u001b[39m=\u001b[39mwait_exponential(\n\u001b[1;32m     51\u001b[0m         multiplier\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     52\u001b[0m         \u001b[39mmin\u001b[39m\u001b[39m=\u001b[39membeddings\u001b[39m.\u001b[39mretry_min_seconds,\n\u001b[1;32m     53\u001b[0m         \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39membeddings\u001b[39m.\u001b[39mretry_max_seconds,\n\u001b[1;32m     54\u001b[0m     ),\n\u001b[1;32m     55\u001b[0m     retry\u001b[39m=\u001b[39m(\n\u001b[0;32m---> 56\u001b[0m         retry_if_exception_type(openai\u001b[39m.\u001b[39;49merror\u001b[39m.\u001b[39mTimeout)\n\u001b[1;32m     57\u001b[0m         \u001b[39m|\u001b[39m retry_if_exception_type(openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mAPIError)\n\u001b[1;32m     58\u001b[0m         \u001b[39m|\u001b[39m retry_if_exception_type(openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mAPIConnectionError)\n\u001b[1;32m     59\u001b[0m         \u001b[39m|\u001b[39m retry_if_exception_type(openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mRateLimitError)\n\u001b[1;32m     60\u001b[0m         \u001b[39m|\u001b[39m retry_if_exception_type(openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mServiceUnavailableError)\n\u001b[1;32m     61\u001b[0m     ),\n\u001b[1;32m     62\u001b[0m     before_sleep\u001b[39m=\u001b[39mbefore_sleep_log(logger, logging\u001b[39m.\u001b[39mWARNING),\n\u001b[1;32m     63\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'error'"
     ]
    }
   ],
   "source": [
    "res = askQuestion(\"What is the amount of uniform live load I should apply to handrail and guardrail systems?\", False)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_hw5_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
